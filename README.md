# LLMs' Sensitivity to Friendly and Unfriendly Behavior
*Kaushika Uppu*

Large language models have become significantly prevalent in recent years, resulting in an explosion of users and usage. With its exponential rise in popularity, LLMs are now being used for everything from coding to homework to essay writing, and many other things in between. Although previous work in this class has shown the extent to which large language models, specifically GPT-2, are sensitive to certain grammatical errors, I was curious about how large language models react to friendliness versus unfriendliness.

The idea of friendly behavior, or behavior that aids someone in some kind of positive way, is known as prosocial behavior. Both empathy and prosocial behavior are known to develop in young children during early childhood, and they are a critical aspect of social interaction and understanding. On the other hand, large language models have grown and expanded substantially in the past few years, and have even been used to model human behavior in certain situations. One study found that LLMs can even simulate one aspect of human social understanding: trust. Therefore, it is possible that large language models are able to replicate facets of social behaviors that are a prominent feature of human development.

With social behavior being such a central component of human society and the idea that large language models are becoming more and more advanced to the point of accurately simulating human behavior, I was interested in seeing whether LLMs would be sensitive to one aspect of this kind of behavior. In this project, I aim to test how much large language models are responsive to the difference between situations in which one acts amicably or not.
